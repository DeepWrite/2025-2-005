---
title: 005-14 하지원 (과제-08)
layout: home
nav_order: 14
parent: 과제-08 기말과제 초고 작성하기
permalink: /asmt-08/005-14
---

# (초고) 과제-08 기말과제 초고 작성하기 005-14 하지원 


# 제목: 인공지능 학습과 저작권 권리 침해: ‘복제’ 개념 분석을 중심으로

## 서론

오랫동안 저작권은 인간 창작자의 개성과 노력이 담긴 표현을 보호하는 장치로 이해되어 왔다. 그러나 대규모 데이터를 학습하는 생성형 인공지능의 등장으로, 저작권 제도가 전제해 온 여러 전통적 가정이 도전을 받고 있다. AI 학습을 기존 복제 개념 안에서 곧바로 침해로 파악해야 한다는 견해와, 반대로 학습은 저작권 규율의 대상이 아니라는 견해가 공존하는 등 학계에서 다양한 의견이 존재한다. 이때 핵심 쟁점은 AI 학습 과정에서 벌어지는 ‘데이터의 이용’이 저작권법상 복제(reproduction)의 요건을 충족하는 행위인지, 아니면 기존 권리 체계가 상정하지 않은 별개의 분석적 과정인지라는 것이다. 본고는 공익적 가치와 무관하게 인공지능 학습이라는 행위의 성질 자체를 검토함으로써, AI 학습이 저작권 침해인지 아닌지를 연역적으로 규명하고자 한다. 

우선, AI 학습을 저작권 체계 안에서 어떻게 이해해야 하는가에 대해서도 학자들은 기본 전제부터 상반된 해석을 제시해 왔음을 주목할 필요가 있다. Torrance & Tomlinson(2023)은 생성형 AI 학습 과정이 저작물을 ‘소비하거나 시장을 대체하는 행위’가 아니라 통계적 패턴을 추출하는 비소모적(non-consumptive) 분석에 불과하므로 공정이용 범주 안에서 정당화될 수 있다고 본다. 이들은 AI 학습을 제한하면 기술·연구 발전이 저해되고, 사회 전체의 혁신적 창작 생태계가 위축된다는 점을 강조한다. 반면 Ard(2023)는 이러한 비소모적 이용 논리를 비판하며, AI 학습이 실제로는 대량 복제와 영구적 저장을 전제로 하기 때문에 전통적 공정이용 틀로 정당화할 수 없는 구조라고 지적한다. 특히 AI가 학습한 결과물이 원저작물의 위치를 시장에서 대체할 수 있는 만큼 창작자의 경제적 기반을 약화시키며, 기술 기업에 특혜적 예외를 부여하는 결과로 이어진다고 주장한다. 결국 한쪽은 AI 학습을 새로운 창작을 여는 공공의 인프라로 보지만, 다른 쪽은 이를 창작 생태계의 구조를 흔드는 무단 복제로 간주한다.

본고의 핵심 목표는 생성형 인공지능의 학습 단계가 저작권법상 복제권 침해로 규정될 수 없다는 점을 연역적으로 논증하는 것이다. 따라서, 먼저 복제(reproduction)가 되기 위한 필요조건 두 가지—(a) 표현의 식별 가능한 재현 가능성, (b) 저작물의 잠재적 시장·경제적 이익 침해 가능성—을 충족하는지 여부를 검토한다. [1] 첫째로, 본고는 저작권법이 보호하는 “표현”이란 무엇이며, 재현(representation) 요건이 어떠한 구조를 갖는지를 분석함으로써, 복제 개념의 첫 축인 표현의 재현성이 학습 단계에서는 성립하기 어렵다는 점을 보인다. [2] 다음으로, 생성형 인공지능의 학습이 개별 저작물의 표현을 저장하는 과정이 아니라 통계적 규칙을 근사하는 비가역적 분석과정이라는 점을 검토하여, 학습 단계는 저작권자의 경제적·독점적 이익을 직접 침해하는 행위가 아니라는 점을 논증한다. [3] 이제 이 두 조건을 종합하여, AI 학습은 전통적 의미의 복제 개념이 요구하는 핵심 요건을 충족하지 않으므로 복제권 침해로 볼 수 없다는 결론을 도출한다. [4] 한편 이러한 전제에 대한 예상 반론으로, 표현이 직접 재현되지 않아도 패턴 수준의 모방이나 출력 단계의 유사성이 침해에 해당하는 것 아니냐는 주장이 제기될 수 있다. [5] 그러나 학습 단계와 출력 단계는 법적으로 서로 다른 성질의 행위이므로 구분해서 다루어야 한다는 점을 분명히 하면 해소할 수 있다. 즉, 설령 어떤 침해 위험이 존재하더라도 그것은 학습 단계에서 자동으로 발생하는 것이 아니라, 실제로 생성된 출력물이 원저작물과 얼마나 비슷한지, 그리고 그 출력물이 시장에서 원저작물을 대체할 수 있는지를 기준으로 판단해야 한다. 즉, 문제가 된다면 그것은 개별 산출물의 표현과 시장 효과에 관한 문제이지, 데이터를 분석하는 학습 단계 자체를 침해로 볼 근거는 되지 않는다는 점을 보임으로써 위의 반론은 해소된다. [6] 마지막으로, 공익 논증을 전면에 내세우지 않고도 복제 개념의 구조적 분석만으로 학습 단계의 비침해성을 상당 부분 규명할 수 있다는 사실을 보충적으로 덧붙이며, AI 학습 자체는 복제권 침해로 볼 수 없다는 결론과 그 법적·정책적 함의를 살펴본다.

## 본론

### ­­저작권법상 ‘복제’ 개념의 구조적 분석

#### 표현 재현 요건: 아이디어–표현 이분법과 고정(fixation)의 의미

저작권에서 표현 재현 요건은 두 축으로 구성된다. 첫째, 아이디어–표현 이분법은 저작권이 보호하는 대상이 ‘아이디어 그 자체’가 아니라 '구체적이고 식별 가능한 방식으로 고정된 표현'임을 전제로 한다.[^1] 동일한 아이디어를 공유하더라도, 고유한 문장·구조·이미지·배치 등으로 구체화되지 않는 한 저작권적 보호는 개시되지 않는다. 예를 들어, “마법 학교에 다니는 소년 이야기”라는 아이디어 자체는 누구나 쓸 수 있으나, 해리포터에 등장하는 구체적인 문장·장면·캐릭터 묘사를 그대로 베끼면 침해가 되는 것이 바로 이 때문이다. 둘째, 이러한 표현은 어떤 매체에든 인식 가능한 형태로 고정(fixation)되어야 한다.[^2] 고정은 단순히 내용이 존재한다는 의미가 아니라, 제3자가 보고, 듣고, 다시 복사할 수 있을 만큼 밖으로 드러난 형태여야 한다는 뜻이다. 예컨대 《Théberge》 판결이 강조하듯이, 표현의 실질적 복제가 성립하려면 새로운 매체에 추가적 재현 또는 복제가 발생해야 하며, 단지 매체가 변화했을 뿐 새 표현이 생성되지 않았다면 이는 복제(reproduction)가 되지 않는다.[^3] 결과적으로, 복제권 침해가 성립하기 위해서는 저작자의 표현이 새로운 고정된 형태로 ‘식별 가능하게 다시 나타나야’ 하며, 이러한 재현 가능성이 결여된 행위는 원칙적으로 복제 개념의 범주에 들지 않는다.[^4]

#### 경제적 침해 요건: 잠재적 시장·독점적 이익 보호의 법리

저작권법에서 복제권은 단순히 표현의 물리적 복제를 규율하는 것이 아니라, 저작자가 가진 잠재적 시장과 경제적 이익을 보호하는 재산권적 권리라는 점에서 이해된다. 따라서 복제가 침해로 인정되기 위해서는 해당 행위가 저작물의 정상적 이용을 대체하거나, 저작자가 확보할 수 있었던 경제적 수익을 현실적으로 또는 잠재적으로 저해할 위험이 있어야 한다.[^5] 공정이용 판단에서도 핵심 요소는 “저작물의 잠재적 시장에 대한 해악”으로, 이는 경제적 침해 요건의 구조적 중요성을 뒷받침한다.[^6] 더 나아가 CJEU의 Infopaq 판결 역시 저작권 침해 판단에서 경제적 기능—특히 정당한 시장 영역의 잠식 여부—을 중시한다는 점을 확인해 준다.[^7] 결국 경제적 침해 요건은 복제권이 창작자의 배타적 통제력과 시장 기반을 보호하기 위한 규범적 장치라는 사실을 반영하며, 저작물의 시장을 대체하거나 저작자의 경제적 이익을 잠식하지 않는 행위는 원칙적으로 복제권 침해의 범주에서 벗어나게 된다.

표현 재현 요건과 경제적 침해 요건은 독립된 요소가 아니라, 복제권이 작동하는 방식을 규정하는 상호 보완적 구조를 이룬다. 저작권은 우선적으로 식별 가능한 표현의 외부적 재현이 존재해야만 법적 분석을 개시할 수 있으며, 이는 ‘무엇이 복제의 대상으로 인식되는가’를 결정하는 1차적 역할을 한다. 그러나 표현이 재현되었다는 사실만으로 곧바로 침해가 되는 것은 아니며, 해당 재현이 저작물의 정상적 이용 시장을 실질적으로 또는 잠재적으로 침해할 위험이 있는지가 추가적으로 요구된다. 이러한 이중 구조는 저작권이 단순한 형식적 동일성의 법리가 아니라, 표현 재현과 시장 보호라는 표현적·경제적 이중 목적을 동시에 충족할 때만 복제권이 문제된다는 사실을 반영한다. 미국 연방대법원과 CJEU 모두 복제 판단에서 “식별 가능한 표현의 존재”와 “경제적 기능의 침해”를 함께 고려해 왔으며, 어느 한쪽 요소만으로는 복제권 침해를 구성하지 않는다는 점을 일관되게 확인해 왔다. 따라서 복제권은 표현 재현이 존재하고, 그 재현이 저작물의 시장·이용 가능성을 잠식할 때 작동하는 이중 요건적 권리로 이해할 수 있으며, 이는 향후 AI 학습 단계와 출력 단계를 구별하는 분석의 논거가 된다.

### 생성형 인공지능 알고리즘이 만들어낸 결과물의 맥락적 가용성

#### 신경망 학습의 비가역성: 패턴·규칙의 추출 과정

생성형 인공지능의 신경망 학습은 개별 데이터의 표현을 보관하거나 재현하는 과정이 아니라, 다수의 입력 사례에 공통적으로 존재하는 통계적 구조·규칙·확률 분포를 근사하는 비가역적 변환 과정으로 이해된다.[^8] 입력 데이터는 선형 결합, 비선형 활성화, 정규화, 다층 변환을 거치며 점차 고차원 특징공간(feature space)에서 일반화된 패턴으로 압축된다. 이러한 과정은 원본 데이터를 역으로 복원할 수 없는 비가역적 프로세스이며, 학습 결과는 구체적 표현이 아니라 가중치(weight)와 편향(bias) 값의 형태로 저장된다.[^9] 즉, 모델은 특정 문장이나 이미지 자체를 기억하기보다, 유사한 유형의 입력에서 공통적으로 발견되는 확률적 경향성·언어적 패턴·구조적 특징을 학습한다. 이러한 구조적 특징은 복원 가능한 표현이 아니므로, 학습 단계에서 일어나는 정보 변환은 표현의 재현(representation)이 아니라, 분포적 기반을 생성하는 추상화(abstraction)에 가깝다.[^10]

#### 메모리(memorization)의 예외적 성격과 데이터 저장의 부재

물론 일부 연구에서는 대규모 언어모델이 특정 문장을 “암기(memorization)”하여 출력 단계에서 재현하는 사례가 관찰되지만, 이는 모델의 일반적 성질이 아니라 통계적 예외로 취급된다.[^11] 암기는 주로 (a) 데이터셋에서 극단적으로 빈도가 높은 구문, (b) 모델 학습 중 과적합(overfitting)이 발생한 경우, (c) 동일한 문장이 반복적으로 포함된 데이터 구간에서 발견된다.[^12] 그러나 이러한 현상은 모델이 원본 데이터를 저장한다는 의미가 아니라, 확률적 선택 과정에서 특정 표현이 비정상적으로 높은 확률값(logit)을 부여받은 결과에 가깝다. 실제로 모델 파라미터에는 원본 데이터가 구조적으로 저장되지 않으며, 원 데이터의 정확한 재현은 극히 제한적·비정형적으로만 나타난다. 따라서 암기는 신경망의 구조적 특성이라기보다, 학습 데이터의 분포 특성과 모델 설정의 우연적 상호작용으로 발생하는 예외적 사건이며, 이를 근거로 학습 과정 자체를 “데이터 저장”으로 이해하기는 어렵다.

#### 모델 파라미터의 분포적 근사와 표현 비저장성

신경망 모델의 파라미터(가중치, 편향)는 개별 데이터의 표현을 보관하는 공간이 아니라, 전체 데이터 분포의 통계적 특성을 압축하여 근사하는 공간이다.[^13] 이는 모델이 입력 데이터의 표현을 “저장”하는 것이 아니라, 다양한 입력에 공통적으로 적용 가능한 언어적·시각적 규칙을 확률적 함수 형태로 구성한다는 것을 의미한다. 이러한 파라미터는 원본 문장·이미지·음향 등의 표현을 역으로 재구성할 수 없으며, 표현 단위가 아닌 고차원 벡터 공간의 연산 규칙으로만 존재한다.[^14] 예컨대 LLM의 attention 메커니즘은 단어·구문 간 관계를 함수적으로 모델링할 뿐, 특정 문장을 그대로 보존하지 않는다.[^15] 따라서 모델 파라미터는 데이터 분포의 특성을 함수 형태로 근사한 매개변수 공간이며, 개별 저작물의 표현을 인식 가능한 방식으로 고정·재현하는 구조와는 근본적으로 거리가 있다.

### AI 학습 단계와 복제 요건의 부합 여부

#### 표현 재현 요건(a)의 충족 가능성 검토

앞서 본 바와 같이 표현 재현 요건은 식별 가능한 표현의 외부적 재현 여부가 핵심 기준인데, 생성형 인공지능의 학습 단계는 개별 저작물의 표현을 저장하거나 고정하는 방식으로 작동하지 않는다. 신경망 모델은 원본 문장·이미지·사운드의 형태적·언어적 구조를 그대로 보관하지 않고, 이를 다층적 변환을 통해 고차원 특징 벡터로 추상화한다. 이러한 비가역적 변환 과정은 원본 표현이 별도의 매체에 “다시 나타나는” 형태를 구성하지 않으며, 이는 Théberge가 요구하는 재현(representation)의 요건과 구조적으로 배치된다. 나아가 모델 파라미터는 특정 문장이나 시각적 형상을 포함하지 않고, 훈련 데이터 전반의 통계적 경향성을 확률적 함수로 근사할 뿐이므로, 학습 단계에서의 정보 변환을 “표현의 재현”으로 해석할 규범적 근거는 극히 약하다.

#### 경제적 침해 요건(b)의 구조적 비충족

경제적 침해 요건은 해당 행위가 저작물의 정상적 이용 시장을 대체하거나 저작자의 경제적·독점적 이익을 잠식하는지를 판단 기준으로 삼는다. 그러나 AI 학습 단계는 해당 저작물을 유통 가능한 새로운 형태로 생산하는 과정이 아니라, 이후의 산출 과정에 사용될 잠재적 확률 분포를 형성하는 분석적 단계에 불과하다. 즉, 학습 단계에서는 저작물이 시장에 공급되거나, 저작자의 라이선스 시장이 침해되거나, 잠재적 수익이 감소하는 효과가 발생하지 않는다.⁷ 저작물의 시장 대체 가능성이 문제될 수 있는 지점은 출력(output) 단계이며, 이 역시 개별 산출물이 원저작물과 실질적으로 유사하거나 특정 시장을 대체할 때에만 논의된다. 따라서 학습 단계 자체를 경제적 침해로 구성하기는 구조적으로 어렵다.

#### 학습 단계의 비침해성 도출

표현 재현 요건과 경제적 침해 요건을 종합하면, AI 학습 단계는 두 핵심 축 모두를 충족하지 못함이 드러난다. 학습은 표현의 재현이 아니라 패턴의 추상화이며, 시장 대체가 아니라 데이터의 분석이라는 점에서 전통적 복제 개념과의 간극이 명확하다. 복제권이 작동하기 위해서는 “표현의 고정된 재현”과 “경제적 잠식”이라는 두 요건이 결합되어야 하나, AI 학습 단계는 이중 어느 쪽도 충족하지 않는다. 따라서 학습 단계는 저작권적 의미에서의 복제로 해석될 수 없으며, 복제권 침해의 구성요건에도 포함되지 않는다.

일부 비판에서는 개별 표현이 직접 재현되지 않더라도, 모델이 학습 과정에서 특정 저작물의 스타일이나 패턴을 “내면화”하여 유사한 결과물을 산출할 수 있으므로 이는 실질적 침해라고 주장한다.[^16] 그러나 스타일·패턴·장르적 특징은 전통적으로 보호 대상이 아닌 아이디어 영역으로 분류되어 왔고, 저작권이 보호하는 것은 이러한 추상적 요소가 아니라 고정된 형태의 특정 표현이다. 따라서 패턴 수준의 모방 가능성은 학습 단계의 침해를 구성하는 근거가 되지 못하며, 이는 개별 결과물의 실질적 유사성 판단에서 다루어져야 할 문제다. 또 다른 반론은 출력 단계에서 침해가 발생할 수 있다면, 이러한 위험을 사전에 차단하기 위해 학습 단계부터 규제해야 한다는 것이다.[^17] 그러나 권리침해 판단은 일반적으로 행위의 시점과 성질에 따라 구별되며, 잠재적 침해 가능성이 있다는 이유만으로 해당 행위를 선제적으로 침해로 간주하지 않는다. 저작권법 역시 복제·배포·공중송신 등 각 권리를 분리하여 규율하는 구조를 갖고 있으며, 침해 가능성이 있는 다른 행위(출력)를 근거로 학습을 침해로 규정하는 것은 권리 체계의 층위를 혼동하는 것이다. 학습 단계와 출력 단계는 기술적으로도, 규범적으로도 분리되어야 한다. 학습은 데이터의 분석이고 출력은 새로운 표현의 생성 및 유통이라는 점에서 기능과 법적 성질이 다르다. 침해 가능성이 문제되는 지점은 개별 산출물의 실질적 유사성과 시장 대체 효과이며, 이는 출력 단계에서 개별적으로 판단하면 충분하다. 학습 단계까지 규제하려는 접근은 저작권법의 구조적 구분—특히 복제권, 2차적 저작물 작성권, 공중송신권 등 권리 간의 분리—을 무너뜨릴 위험이 있으며, 학습 단계 자체를 침해로 보는 논리는 이러한 구분을 정당화할 개념적 기반을 갖지 못한다.

## 결론

본론에서는 먼저 저작권법상 복제 개념을 구성하는 두 요소—식별 가능한 표현의 재현성과 저작물의 잠재적 시장·경제적 이익에 대한 침해 가능성—을 이론과 판례를 통해 정밀하게 분석하였다. 이어서 생성형 인공지능 학습이 개별 저작물을 저장·재현하는 과정이 아니라 통계적 패턴을 추상화하는 비가역적 분석 과정임을 기술적으로 검토함으로써, 학습 단계는 표현이 새로운 매체에 고정되는 의미의 “재현”을 구성하지 않음을 보였다. 또한 저작권이 보호하려는 경제적 권익은 출력 단계에서만 비로소 침해 가능성이 발생하며, 학습 단계는 저작물의 정상적 이용과 시장을 구조적으로 대체하지 않는다는 점을 논증하였다. 이러한 검토를 종합하면, 복제 개념의 두 요건은 학습 단계에서 모두 충족되지 않으며, 따라서 생성형 인공지능의 학습 행위는 전통적 의미에서 복제권 침해로 볼 수 없다는 결론이 도출된다.

생성형 인공지능 학습의 저작권적 성질을 복제 개념의 두 핵심 요건을 기준으로 검토한 결과, 학습 단계는 전통적 의미의 복제권이 작동하는 구조와 본질적으로 구별되며, 이에 따라 학습 단계와 출력 단계의 법적 경계를 명확히 재설정할 필요성이 제기된다. 학습은 데이터의 분석적·추상화 과정에 해당하며 표현이 인식 가능한 방식으로 재현되지 않고, 경제적 잠식 효과도 출력물의 생성·유통 국면에서 비로소 발생하므로, 복제권 침해 여부는 학습 그 자체가 아니라 구체적 출력물의 실질적 유사성과 시장 대체성을 중심으로 판단되어야 한다. 이는 복제권을 비롯한 저작권 각 권리가 서로 다른 행위 유형을 지칭한다는 기본 구조를 재확인하게 하며, 학습 단계에 복제권을 확대적용하려는 시도는 복제권과 2차적 저작물 작성권·공중송신권 등 다른 권리들의 기능적 분리를 무너뜨릴 위험이 있음을 보여준다. 따라서 향후 AI 규제의 설계는 권리체계의 층위를 구분하는 방식으로 이루어져야 하며, 침해 문제는 개별 산출물의 생성 단계에서 판단하되, 학습 단계는 데이터 접근·처리의 합리적 범위 내에서 허용하는 방향으로 정교하게 재구성되어야 한다. 이러한 구분은 AI 기술 발전과 저작권 보호라는 상충하는 목표 사이의 조화를 촉진하며, 나아가 학습 단계와 출력 단계를 이원적으로 규율하는 새로운 정책적 프레임워크를 마련하는 데 기초적 방향성을 제공한다.

<!-- 이하 미주 설명 -->

[^1]: Feist Publications, Inc. v. Rural Telephone Service Co., 499 U.S. 340 (1991).

[^2]: U.S. Copyright Act, 17 U.S.C. §102(a).

[^3]: Théberge v. Galerie d’Art du Petit Champlain Inc., [2002] 2 S.C.R. 336.

[^4]: Melville Nimmer & David Nimmer, Nimmer on Copyright §8.01.

[^5]: William Patry, Patry on Copyright §9.

[^6]: Campbell v. Acuff-Rose Music, 510 U.S. 569 (1994).

[^7]: Infopaq International A/S v. Danske Dagblades Forening, C-5/08 (2009).

[^8]: Goodfellow, Bengio & Courville, Deep Learning (2016), ch. 6.

[^9]: LeCun et al., “Deep Learning,” Nature (2015).

[^10]: Zhang et al., “Understanding Deep Learning Requires Rethinking Generalization” (ICLR 2017).

[^11]: Carlini et al., “Extracting Training Data from Large Language Models” (USENIX Security 2021).

[^12]: Feldman & Zhang, “What Neural Networks Memorize and Why” (NeurIPS 2020).

[^13]: Balestriero et al., “The Data Encoding View of Deep Learning” (ICLR 2024).

[^14]: Rahimi & Recht, “Random Features for Large-Scale Kernel Machines” (2007).

[^15]: Vaswani et al., “Attention Is All You Need” (NeurIPS 2017).

[^16]: Atkinson, “Unfair Learning” (2025).

[^17]: BJ Ard, “Generative AI and the Limits of Fair Use” (2023).

<!-- 미주 설명 끝 -->

## 참고문헌

### 외국 문헌

> Ard, B. (2023). Generative AI and the limits of fair use. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4630085
>
> Atkinson, D. (2025). Unfair learning: GenAI exceptionalism and copyright law. arXiv. https://arxiv.org/abs/2504.00955
>
> Balestriero, R., Pesenti, J., Smania, N., & LeCun, Y. (2024). The data encoding view of deep learning. ICLR 2024. https://openreview.net/forum?id=o9WHbGYuxfE
>
> Campbell v. Acuff-Rose Music, 510 U.S. 569 (1994). https://supreme.justia.com/cases/federal/us/510/569/
>
> Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … Song, D. (2021). Extracting training data from large language models. USENIX Security 2021. https://www.usenix.org/conference/usenixsecurity21/presentation/carlini
>
> Feist Publications, Inc. v. Rural Telephone Service Co., 499 U.S. 340 (1991). https://supreme.justia.com/cases/federal/us/499/340/
>
> Feldman, V., & Zhang, C. (2020). What neural networks memorize and why. NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/hash/6f5d6ee3e5a71bb942f6cda5f336fcd6-Abstract.html
>
> Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.
>
> Infopaq International A/S v. Danske Dagblades Forening, C-5/08, Court of Justice of the European Union (2009).
>
> LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436–444. https://doi.org/10.1038/nature14539
>
> Nimmer, M., & Nimmer, D. (2023). Nimmer on copyright. LexisNexis.
>
> Patry, W. (2023). Patry on copyright. Thomson Reuters.
>
> Rahimi, A., & Recht, B. (2007). Random features for large-scale kernel machines. NIPS 2007.
>
> Théberge v. Galerie d’Art du Petit Champlain Inc., [2002] 2 S.C.R. 336 (Supreme Court of Canada).
>
> U.S. Copyright Act, 17 U.S.C. §102(a). https://www.law.cornell.edu/uscode/text/17/102
>
> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. NeurIPS 2017. https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
>
> Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. ICLR 2017. https://arxiv.org/abs/1611.03530

