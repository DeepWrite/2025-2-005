---
title: 005-14 하지원 (과제-09)
layout: home
nav_order: 14
parent: 과제-09 기말과제 최종본
permalink: /asmt-09/005-14
---

# (최종본) 과제-09 기말과제 005-14 하지원 


# 제목: 인공지능 학습 단계에서의 저작권 권리 침해: ‘복제’ 개념 분석을 중심으로

## 서론

오랫동안 저작권은 인간 창작자의 개성과 노력이 담긴 표현을 보호하는 장치로 이해되어 왔다. 그러나 대규모 데이터를 학습하는 생성형 인공지능의 등장으로, 저작권 제도가 전제해 온 여러 전통적 가정이 도전을 받고 있다. 특히 AI 학습을 기존 복제 개념 안에서 곧바로 침해로 파악해야 한다는 견해와, 반대로 학습은 복제가 아니므로 저작권 규율의 대상이 아니라는 견해가 공존하는 등 AI 학습의 저작권과 관련한 논쟁이 존재한다. 이때 핵심 쟁점은 AI 학습 과정에서 벌어지는 ‘데이터의 이용’이 저작권법상 복제(reproduction)의 요건을 충족하는 행위인지, 아니면 기존 권리 체계가 상정하지 않은 별개의 분석적 과정인지라는 것이다. 본고는 공익적 가치와 무관하게 인공지능 학습이라는 행위의 성질 자체를 검토함으로써, AI 학습이 저작권 침해인지 아닌지를 연역적으로 규명하고자 한다. 

우선, AI 학습을 저작권 체계 안에서 어떻게 이해해야 하는가에 대해서도 학자들은 기본 전제부터 상반된 해석을 제시했음을 주목해야 한다. Torrance & Tomlinson(2023)은 생성형 AI 학습 과정이 저작물을 ‘소비하거나 시장을 대체하는 행위’가 아니라 통계적 패턴을 추출하는 비소모적(non-consumptive) 분석에 불과하므로 공정이용 범주 안에서 정당화될 수 있다고 본다. 이들은 AI 학습을 제한하면 기술·연구 발전이 저해되고, 사회 전체의 혁신적 창작 생태계가 위축된다는 점을 강조한다. 반면 Ard(2023)는 이러한 비소모적 이용 논리를 비판하며, AI 학습이 실제로는 대량 복제와 영구적 저장을 전제로 하기 때문에 전통적 공정이용 틀로 정당화할 수 없는 구조라고 지적한다. 특히 AI가 학습한 결과물이 원저작물의 위치를 시장에서 대체할 수 있는 만큼 창작자의 경제적 기반을 약화시키며, 기술 기업에 특혜적 예외를 부여하는 결과로 이어진다고 주장한다. 그러나 이 논쟁은 대부분 “공정이용 여부”에 집중되어 있으며, 정작 저작권법이 전제하는 복제 개념의 구조 그 자체에 대한 체계적인 검토는 상대적으로 부족하다. 본 논문은 이러한 공백을 보완하기 위해 복제의 성립 요건을 구조적으로 분석한 뒤, AI 학습 단계가 이 요건을 충족하는지 연역적으로 평가한다.

본고의 핵심 목표는 생성형 인공지능의 학습 단계가 저작권법상 복제권 침해로 규정될 수 없다는 점을 연역적으로 논증하는 것이다. 따라서, 복제(reproduction)가 되기 위한 필요조건 두 가지—(a) 표현의 식별 가능한 재현 가능성, (b) 저작물의 잠재적 시장·경제적 이익 침해 가능성—을 충족하는지 여부를 검토한다. [1] 첫째로, 본고는 저작권법이 보호하는 “표현”이란 무엇이며, 재현(representation) 요건이 어떠한 구조를 갖는지를 분석하고, 저작권법에서의 경제적 침해 요건에 대해 살펴본다. [2] 다음으로, 복제 개념의 첫 축인 표현의 재현성이 학습 단계에서는 성립하기 어려우며, 학습 단계는 저작권자의 경제적·독점적 이익을 직접 침해하는 행위가 아니라는 점을 논증한다. [3] 이제 이 두 전제를 종합하여, AI 학습은 전통적 의미의 복제 개념이 요구하는 핵심 요건을 충족하지 않으므로 복제권 침해로 볼 수 없다는 결론을 도출한다. [4] 한편 이러한 전제에 대한 예상 반론으로, 표현이 직접 재현되지 않아도 패턴 수준의 모방이나 출력 단계의 유사성이 침해에 해당하는 것 아니냐는 주장이 제기될 수 있다. [5] 그러나 학습 단계와 출력 단계는 법적으로 서로 다른 성질의 행위이므로 구분해서 다루어야 한다는 점을 분명히 하면 해소할 수 있다. 즉, 설령 어떤 침해 위험이 존재하더라도 그것은 학습 단계에서 자동으로 발생하는 것이 아니라, 실제로 생성된 출력물이 원저작물과 얼마나 비슷한지, 그리고 그 출력물이 시장에서 원저작물을 대체할 수 있는지를 기준으로 판단된다. 즉, 문제가 된다면 그것은 개별 산출물의 표현과 시장 효과에 관한 문제이지, 데이터를 분석하는 학습 단계 자체를 침해로 볼 근거는 되지 않는다는 점을 보임으로써 위의 반론은 해소된다. [6] 마지막으로, 공익 논증을 전면에 내세우지 않고도 복제 개념의 구조적 분석만으로 학습 단계의 비침해성을 상당 부분 규명할 수 있다는 사실을 보충적으로 덧붙이며, "생성형 인공지능의 학습 단계는 저작권법상 복제에 해당하지 않으며, 따라서 복제권 침해로 구성될 수 없다"는 결론을 도출하고 그 법적·정책적 함의를 살펴본다.

## 본론

### ­­저작권법상 ‘복제’ 개념의 구조적 분석

#### 표현 재현 요건: 아이디어–표현 이분법과 고정(fixation)의 의미

저작권에서 표현 재현 요건은 두 조건으로 구성된다. 첫째, 아이디어–표현 이분법은 저작권이 보호하는 대상이 ‘아이디어 그 자체’가 아니라 '구체적이고 식별 가능한 방식으로 고정된 표현'임을 전제로 한다.[^1] 동일한 아이디어를 공유하더라도, 고유한 문장·구조·이미지·배치 등으로 구체화되지 않는 한 저작권적 보호는 개시되지 않는다. 예를 들어, “마법 학교에 다니는 소년 이야기”라는 아이디어 자체는 누구나 쓸 수 있으나, 해리포터에 등장하는 구체적인 문장, 장면, 캐릭터 묘사를 그대로 베끼면 침해가 되는 것이 바로 이 때문이다. 둘째, 이러한 표현은 어떤 매체에든 인식 가능한 형태로 고정(fixation)되어야 한다.[^2] 고정은 단순히 내용이 존재한다는 의미가 아니라, 제3자가 보고, 듣고, 다시 복사할 수 있을 만큼 밖으로 드러난 형태여야 한다는 뜻이다. 예컨대 《Théberge》 판결이 강조하듯이, 표현의 실질적 복제가 성립하려면 새로운 매체에 추가적 재현 또는 복제가 발생해야 하며, 단지 매체가 변화했을 뿐 새 표현이 생성되지 않았다면 이는 복제(reproduction)가 되지 않는다.[^3] 결과적으로, 복제권 침해가 성립하기 위해서는 저작자의 표현이 새로운 고정된 형태로 ‘식별 가능하게 다시 나타나야’ 하며, 이러한 재현 가능성이 결여된 행위는 원칙적으로 복제 개념의 범주에 들지 않는다.[^4]

#### 경제적 침해 요건: 잠재적 시장·독점적 이익 보호의 법리

저작권법에서 복제권은 단순히 표현의 물리적 복제를 규율하는 것이 아니라, 저작자가 가진 잠재적 시장과 경제적 이익을 보호하는 재산권적 권리라는 점에서 이해된다. 따라서 복제가 침해로 인정되기 위해서는 해당 행위가 저작물의 정상적 이용을 대체하거나, 저작자가 확보할 수 있었던 경제적 수익을 현실적으로 또는 잠재적으로 저해할 위험이 있어야 한다.[^5] 공정이용 판단에서도 핵심 요소는 “저작물의 잠재적 시장에 대한 해악”으로, 이는 경제적 침해 요건의 구조적 중요성을 뒷받침한다.[^6] 더 나아가 CJEU의 Infopaq 판결 역시 저작권 침해 판단에서 경제적 기능—특히 정당한 시장 영역의 잠식 여부—을 중시한다는 점을 확인해 준다.[^7] 결국 경제적 침해 요건은 복제권이 창작자의 배타적 권리와 시장 기반을 보호하기 위한 수단이라는 사실을 반영하며, 저작물의 시장을 대체하거나 저작자의 경제적 이익을 잠식하지 않는 행위는 원칙적으로 복제권 침해의 범주에서 벗어나게 된다.

표현 재현 요건과 경제적 침해 요건은 독립된 요소가 아니라, 복제권이 작동하는 방식을 규정하는 상호 보완적 구조를 이룬다. 저작권은 우선적으로 식별 가능한 표현의 외부적 재현이 존재해야만 법적 분석을 개시할 수 있으며, 이는 ‘무엇이 복제의 대상으로 인식되는가’를 결정하는 1차적 역할을 한다. 그러나 표현이 재현되었다는 사실만으로 곧바로 침해가 되는 것은 아니며, 해당 재현이 저작물의 정상적 이용 시장을 실질적으로 또는 잠재적으로 침해할 위험이 있는지가 추가적으로 요구된다. 이러한 이중 구조는 저작권이 단순한 형식적 동일성의 법리가 아니라, 표현 재현과 시장 보호라는 표현적·경제적 이중 목적을 동시에 충족할 때만 복제권이 문제된다는 사실을 반영한다. 미국 연방대법원과 CJEU 모두 복제 판단에서 “식별 가능한 표현의 존재”와 “경제적 기능의 침해”를 함께 고려해 왔으며, 어느 한쪽 요소만으로는 복제권 침해를 구성하지 않는다는 점을 일관되게 확인해 왔다. 따라서 복제권은 표현 재현이 존재하고, 그 재현이 저작물의 시장을 잠식할 때 작동하는 이중 요건적 권리로 이해할 수 있으며, 이는 향후 AI 학습 단계와 출력 단계를 구별하는 분석의 논거가 된다.

### AI 학습 과정의 비재현적 구조 및 경제적 침해 가능성

#### 신경망 학습의 구조적 성질: 원본 재현 불가능성

생성형 인공지능의 신경망 학습은 개별 데이터의 표현을 보관하거나 재현하는 과정이 아니라, 다수의 입력 사례에 공통적으로 존재하는 통계적 구조, 규칙, 확률 분포를 근사하는 비가역적 변환 과정으로 이해된다.[^8] 입력 데이터는 선형 결합, 비선형 활성화, 정규화, 다층 변환을 거치며 점차 고차원 특징공간(feature space)에서 일반화된 패턴으로 압축된다. 이러한 과정은 원본 데이터를 역으로 복원할 수 없는 비가역적 프로세스이며, 학습 결과는 구체적 표현이 아니라 가중치(weight)와 편향(bias) 값의 형태로 저장된다.[^9] 즉, 모델은 특정 문장이나 이미지 자체를 기억하기보다, 유사한 유형의 입력에서 공통적으로 발견되는 확률적 경향성, 언어적 패턴, 구조적 특징을 학습한다. 이러한 구조적 특징은 복원 가능한 표현이 아니므로, 학습 단계에서 일어나는 정보 변환은 표현의 재현(representation)이 요구하는 수준을 충족하지 않으며, 분포적 기반을 생성하는 추상화(abstraction)에 가깝다.[^10] 따라서 생성형 인공지능의 학습 단계는 개별 저작물의 표현을 식별 가능한 형태로 저장하거나 재현하는 행위가 아니라, 표현 이전의 통계적 규칙을 추상화하는 과정에 해당한다. 이와 같은 비가역적 변환 구조는 학습 단계에서 ‘표현의 재현’이 성립하기 어렵다는 법적 결론으로 이어진다.

#### 암기현상(memorization)의 예외적 성격

물론 일부 연구에서는 대규모 언어모델이 특정 문장을 “암기(memorization)”하여 출력 단계에서 재현하는 사례가 관찰되지만, 이는 모델의 일반적 성질이 아니라 통계적 예외로 취급된다.[^11] 암기는 주로 (a) 데이터셋에서 극단적으로 빈도가 높은 구문, (b) 모델 학습 중 과적합(overfitting)이 발생한 경우, (c) 동일한 문장이 반복적으로 포함된 데이터 구간에서 발견된다.[^12] 그러나 이러한 현상은 모델이 원본 데이터를 저장한다는 의미가 아니라, 확률적 선택 과정에서 특정 표현이 비정상적으로 높은 확률값(logit)을 부여받은 결과에 가깝다. 실제로 모델 파라미터에는 원본 데이터가 구조적으로 저장되지 않으며, 원 데이터의 정확한 재현은 극히 제한적, 비정형적으로만 나타난다. 따라서 암기는 신경망의 구조적 특성이라기보다, 학습 데이터의 분포 특성과 모델 설정의 우연적 상호작용으로 발생하는 예외적 사건이며, 이를 근거로 학습 과정 자체를 “데이터 저장”으로 이해하기는 어렵다. 결국 일부 암기 사례의 존재만으로 신경망 학습 전반을 ‘표현의 저장 또는 재현’으로 일반화할 수는 없으며, 이는 학습 단계 자체를 복제 행위로 평가하기 어렵다는 점을 오히려 확인해 준다. 이러한 예외적 현상은 출력 단계의 개별 산출물 평가와 구별되어야 한다.

#### AI 학습 단계의 경제적 영향

이러한 비재현적, 비저장적 구조는 단지 표현 재현 요건을 충족시키지 못하게 할 뿐 아니라, 학습 단계가 저작물의 정상적 이용 시장을 침해할 가능성도 구조적으로 제거한다. 위와 같은 인공지능의 구조로 인해, 학습 단계는 원저작물과 기능적으로 동등한 대체물을 생산할 수 없다. 학습 과정에서 저장되는 것은 개별 표현이 아니라 통계적 패턴의 근사값에 불과하므로, 이는 시장에서 거래되거나 원저작물의 라이선스 시장을 잠식할 수 있는 유형의 산출물이 아니다. 경제적 침해 가능성은 모델이 실제로 표현을 생성하는 출력 단계에서 비로소 발생하며, 학습 단계 자체는 저작권자의 경제적·독점적 이익과 경쟁 관계에 들어갈 여지가 없다.

### AI 학습 단계와 복제 요건의 부합 여부

#### 표현 재현 요건(a)의 충족 가능성 검토

앞서 본 바와 같이 표현 재현 요건은 식별 가능한 표현의 외부적 재현 여부가 핵심 기준인데, 생성형 인공지능의 학습 단계는 개별 저작물의 표현을 저장하거나 고정하는 방식으로 작동하지 않는다. 신경망 모델은 원본 문장, 이미지, 사운드의 형태적, 언어적 구조를 그대로 보관하지 않고, 이를 고차원 특징 벡터로 추상화한다. 이러한 비가역적 변환 과정은 원본 표현이 별도의 매체에 “똑같이 다시 나타나는” 형태를 구성하지 않으며, 이는 Théberge가 요구하는 재현(representation)의 요건과 구조적으로 배치된다. 나아가 모델 파라미터는 특정 문장이나 시각적 형상을 포함하지 않고, 훈련 데이터 전반의 통계적 경향성을 확률적 함수로 근사할 뿐이므로, 학습 단계에서의 정보 변환을 “표현의 재현”으로 해석할 규범적 근거는 극히 약하다.

#### 경제적 침해 요건(b)의 충족 가능성 검토

경제적 침해 요건은 해당 행위가 저작물의 정상적 이용 시장을 대체하거나 저작자의 경제적, 독점적 이익을 잠식하는지를 판단 기준으로 삼는다. 그러나 AI 학습 단계는 해당 저작물을 유통 가능한 새로운 형태로 생산하는 과정이 아니라, 이후의 산출 과정에 사용될 잠재적 확률 분포를 형성하는 분석적 단계에 불과하다. 즉, 학습 단계에서는 저작물이 시장에 공급되거나, 저작자의 라이선스 시장이 침해되거나, 잠재적 수익이 감소하는 효과가 발생하지 않는다. 저작물의 시장 대체 가능성이 문제될 수 있는 지점은 출력(output) 단계이며, 이 역시 개별 산출물이 원저작물과 실질적으로 유사하거나 특정 시장을 대체할 때에만 논의된다. 따라서 학습 단계 자체를 경제적 침해로 구성하기는 구조적으로 어렵다.

#### 학습 단계의 비침해성 도출

표현 재현 요건과 경제적 침해 요건을 종합하면, AI 학습 단계는 두 핵심 축 모두를 충족하지 못함이 드러난다. 학습은 표현의 재현이 아니라 패턴의 추상화이며, 시장 대체를 하지 않는 데이터의 분석이라는 점에서 전통적 복제 개념과의 차이가 명확하다. 복제권이 작동하기 위해서는 “표현의 고정된 재현”과 “경제적 잠식”이라는 두 요건이 결합되어야 하나, AI 학습 단계는 이중 어느 쪽도 충족하지 않는다. 따라서 학습 단계는 저작권적 의미에서의 복제로 해석될 수 없으며, 복제권 침해의 구성요건에도 포함되지 않는다.

일부 비판에서는 출력 단계에서 침해가 발생할 수 있다면, 이러한 위험을 사전에 차단하기 위해 학습 단계부터 규제해야 한다고 주장한다.[^16] 그러나 권리침해 판단은 일반적으로 행위의 시점과 성질에 따라 구별되며, 잠재적 침해 가능성이 있다는 이유만으로 해당 행위를 선제적으로 침해로 간주하지 않는다. 저작권법 역시 복제, 배포, 공중송신 등 각 권리를 분리하여 규율하는 구조를 갖고 있으며, 침해 가능성이 있는 다른 행위(출력)를 근거로 학습을 침해로 규정하는 것은 권리 체계의 층위를 혼동하는 것이다. 학습 단계와 출력 단계는 기술적으로도, 규범적으로도 분리되어야 한다. 학습은 데이터의 분석이고 출력은 새로운 표현의 생성 및 유통이라는 점에서 기능과 법적 성질이 다르다. 침해 가능성이 문제되는 지점은 개별 산출물의 실질적 유사성과 시장 대체 효과이며, 이는 출력 단계에서 개별적으로 판단하면 충분하다. 학습 단계까지 규제하려는 접근은 저작권법의 구조적 구분—특히 복제권, 2차적 저작물 작성권, 공중송신권 등 권리 간의 분리—을 무너뜨릴 위험이 있으며, 학습 단계 자체를 침해로 보는 논리는 이러한 구분을 정당화할 개념적 기반을 갖지 못한다.

## 결론

본론에서는 먼저 저작권법상 복제 개념을 구성하는 두 요소—식별 가능한 표현의 재현성과 저작물의 잠재적 시장·경제적 이익에 대한 침해 가능성—을 이론과 판례를 통해 분석하였다. 이어서 생성형 인공지능 학습이 개별 저작물을 저장·재현하는 과정이 아니라 통계적 패턴을 추상화하는 비가역적 분석 과정임을 기술적으로 검토함으로써, 학습 단계는 표현이 새로운 매체에 고정되는 의미의 “재현”을 구성하지 않음을 보였다. 또한 저작권이 보호하려는 경제적 권익은 출력 단계에서만 비로소 침해 가능성이 발생하며, 학습 단계는 저작물의 정상적 이용과 시장을 구조적으로 대체하지 않는다는 점을 논증하였다. 이러한 검토를 종합하면, 복제 개념의 두 요건은 학습 단계에서 모두 충족되지 않으며, 따라서 생성형 인공지능의 학습 행위는 전통적 의미에서 복제권 침해로 볼 수 없다는 결론이 도출된다.

이러한 결론은 기존의 논쟁에 대해 몇 가지 중요한 기여를 제공한다. 첫째, 지금까지의 논의는 AI 학습을 둘러싼 정당화 여부를 주로 공정이용 논리와 대량복제·시장잠식 논리 사이의 대립 구도에서만 다뤄 왔지만, 본 연구는 이 대립을 저작권법상 ‘복제 개념’ 그 자체의 구조적 요건을 재해석하는 방식으로 재배치하였다. 즉, 학습 단계가 복제인지의 여부는 공정성을 논의하기 이전에, 복제가 성립하기 위한 기본 요건을 충족하는가라는 보다 근본적 차원의 문제임을 보여준다. 둘째, 기존 논쟁은 학습 단계와 출력 단계의 차이를 충분히 구분하지 못한 채 양자를 포괄적으로 판단하는 경향이 있었는데, 본 연구는 두 단계의 법적·기능적 성질을 엄밀히 분리함으로써, 침해 판단의 대상은 학습이 아니라 개별 출력물임을 명확히 했다. 이로써 ‘학습 자체가 침해인가?’라는 논점이 사실상 개념적 혼동에서 비롯된 것임을 밝히고, 논쟁의 기준을 보다 정교하게 정립한다. 셋째, 학습 단계의 비침해성을 복제 개념의 이중 요건을 통해 논증함으로써, AI 학습의 적법성 여부를 기술적 구현 방식이나 산업적 효과에 의존하지 않고 저작권 체계 내부의 개념 논리로부터 도출할 수 있음을 제시하였다.

결론적으로, 학습 단계는 데이터의 분석·추상화 과정에 해당하며 표현이 인식 가능한 방식으로 재현되지 않고, 경제적 잠식 효과도 출력물의 생성 및 유통 단계에서 비로소 발생하므로, 복제권 침해 여부는 학습 그 자체가 아니라 구체적 출력물의 실질적 유사성과 시장 대체성을 중심으로 판단되어야 한다. 이는 복제권을 비롯한 저작권의 각 권리가 서로 다른 행위 단계를 지칭한다는 기본 구조를 재확인하게 하며, 학습 단계에 복제권을 확대적용하려는 시도는 복제권과 2차적 저작물 작성권, 공중송신권 등 권리들 사이의 기능적 분리를 무너뜨릴 위험이 있음을 보여준다. 따라서 향후 AI 규제의 설계는 권리체계의 층위를 구분하는 방식으로 이루어져야 할 것이다. 이러한 구분은 AI 기술 발전과 저작권 보호라는 상충하는 목표 사이의 조화를 촉진할 뿐 아니라, 기존 논쟁이 개념적으로 처리하지 못했던 쟁점을 해소하며 학습 단계와 출력 단계를 이원적으로 규율하는 새로운 정책적·이론적 프레임워크를 마련하는 데 기초적 방향성을 제공한다.

<!-- 이하 미주 설명 -->

[^1]: Feist Publications, Inc. v. Rural Telephone Service Co., 499 U.S. 340 (1991).

[^2]: U.S. Copyright Act, 17 U.S.C. §102(a).

[^3]: Théberge v. Galerie d’Art du Petit Champlain Inc., [2002] 2 S.C.R. 336(캐나다 대법원) 사건은, 작가의 작품이 인쇄된 포스터를 구매한 갤러리가 종이 포스터의 잉크층을 캔버스로 옮기는 ‘캔버스 전사(canvas transfer)’ 기법을 사용해 판매한 행위가 저작권법상 복제(reproduction)에 해당하는지 여부를 다루었다. 대법원은 저작권의 핵심 기능을 저작물의 복사본 수가 증가하는 것을 통제하는 데 있다고 보면서, 단순한 매체 변경이나 물리적 이전이 새로운 고정된 복사본을 추가로 생성하지 않는 경우에는 복제로 볼 수 없다고 판시하였다. 이 판결은 복제 성립 여부를 판단함에 있어, 표현이 식별 가능한 형태로 새로운 고정물에 다시 나타났는지가 핵심 기준임을 분명히 한다.

[^4]: Melville Nimmer & David Nimmer, Nimmer on Copyright §8.01.

[^5]: William Patry, Patry on Copyright §9.

[^6]: Campbell v. Acuff-Rose Music, 510 U.S. 569 (1994).

[^7]: Infopaq International A/S v. Danske Dagblades Forening, C-5/08 (2009).

[^8]: Goodfellow, Bengio & Courville, Deep Learning (2016), ch. 6.

[^9]: LeCun et al., “Deep Learning,” Nature (2015).

[^10]: Zhang et al., “Understanding Deep Learning Requires Rethinking Generalization” (ICLR 2017).

[^11]: Carlini et al., “Extracting Training Data from Large Language Models” (USENIX Security 2021).

[^12]: Feldman & Zhang, “What Neural Networks Memorize and Why” (NeurIPS 2020).

[^13]: Balestriero et al., “The Data Encoding View of Deep Learning” (ICLR 2024).

[^14]: Rahimi & Recht, “Random Features for Large-Scale Kernel Machines” (2007).

[^15]: Vaswani et al., “Attention Is All You Need” (NeurIPS 2017).

[^16]: BJ Ard, “Generative AI and the Limits of Fair Use” (2023).

<!-- 미주 설명 끝 -->

## 참고문헌

### 외국 문헌

> Ard, B. (2023). Generative AI and the limits of fair use. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4630085
>
> Atkinson, D. (2025). Unfair learning: GenAI exceptionalism and copyright law. arXiv. https://arxiv.org/abs/2504.00955
>
> Balestriero, R., Pesenti, J., Smania, N., & LeCun, Y. (2024). The data encoding view of deep learning. ICLR 2024. https://openreview.net/forum?id=o9WHbGYuxfE
>
> Campbell v. Acuff-Rose Music, 510 U.S. 569 (1994). https://supreme.justia.com/cases/federal/us/510/569/
>
> Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … Song, D. (2021). Extracting training data from large language models. USENIX Security 2021. https://www.usenix.org/conference/usenixsecurity21/presentation/carlini
>
> Feist Publications, Inc. v. Rural Telephone Service Co., 499 U.S. 340 (1991). https://supreme.justia.com/cases/federal/us/499/340/
>
> Feldman, V., & Zhang, C. (2020). What neural networks memorize and why. NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/hash/6f5d6ee3e5a71bb942f6cda5f336fcd6-Abstract.html
>
> Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.
>
> Infopaq International A/S v. Danske Dagblades Forening, C-5/08, Court of Justice of the European Union (2009).
>
> LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436–444. https://doi.org/10.1038/nature14539
>
> Nimmer, M., & Nimmer, D. (2023). Nimmer on copyright. LexisNexis.
>
> Patry, W. (2023). Patry on copyright. Thomson Reuters.
>
> Rahimi, A., & Recht, B. (2007). Random features for large-scale kernel machines. NIPS 2007.
>
> Théberge v. Galerie d’Art du Petit Champlain Inc., [2002] 2 S.C.R. 336 (Supreme Court of Canada).
>
> U.S. Copyright Act, 17 U.S.C. §102(a). https://www.law.cornell.edu/uscode/text/17/102
>
> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. NeurIPS 2017. https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
>
> Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. ICLR 2017. https://arxiv.org/abs/1611.03530